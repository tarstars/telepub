<!doctype html>
<html lang="en"><head>
<meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1">
<title>My Math Article</title>
</head><body>
<article>
<h1>My Math Article</h1>
<p>Here’s a lightly edited version with the same meaning:</p>
<hr>
<p><strong>Forging the Algorithm</strong>
Arseniy</p>
<p><strong>Backstory</strong></p>
<p>In earlier posts I talked about my time at Equifax, where I encountered instability in credit-scoring models. I outlined the main ideas behind feature engineering and target construction for this task, and I wrote the largest PL/SQL script I’ve ever written.</p>
<p><strong>Forging an Algorithm</strong></p>
<p>After a while I joined HeadHunter. Things stabilized and the work began. Two or three times a week, after a full day at the office, I would travel from Alekseevskaya to Park Kultury in Moscow (about 45 minutes). There, my friend and I worked on gradient-boosted decision trees with extrapolation. We started by deriving the standard gradient-boosted decision-trees algorithm.</p>
<p>The core procedure is splitting. Let’s unpack it step by step. First, we need to choose a loss function to minimize. Suppose our model produces a score s_i for object i whose true label is y_i. The loss for this object is l(s_i, y_i), and the total loss is L=\sum_{i=1}^{n} l(s_i, y_i).</p>
<figure class="eq"><img src="/telepub/assets/eq-4df71b1494e2.svg" alt="\mathcal{L}(\theta) = \sum_{i=1}^{n} l(s_\theta(x_i), y_i)"></figure>
</article>
</body></html>