<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Forging the Algorithm</title>
</head>
<body>
<article>
<p><strong>Forging the Algorithm</strong> Arseniy</p>
<p><strong>Backstory</strong></p>
<p>In earlier posts I talked about my time at Equifax, where I encountered instability in credit-scoring models. I outlined the main ideas behind feature engineering and target construction for this task, and I wrote the largest PL/SQL script I’ve ever written.</p>
<p><strong>Forging an Algorithm</strong></p>
<p>After a while I joined HeadHunter. Things stabilized and the work began. Two or three times a week, after a full day at the office, I would travel from Alekseevskaya to Park Kultury in Moscow (about 45 minutes). There, my friend and I worked on gradient-boosted decision trees with extrapolation. We started by deriving the standard gradient-boosted decision-trees algorithm.</p>
<p>The core procedure is splitting. Let’s unpack it step by step. First, we need to choose a loss function to minimize. Suppose our model produces a score s for object i whose true label is y. The loss for this object is l(s, y), and the total loss is:</p>
<figure class="eq-block">
  <img src="https://tarstars.github.io/telepub/good_looking/assets/eq-main_v314.png" alt="L = \sum_{i=1}^{n} l(s_i, y_i)" />
</figure>
<p>Now we have a set of records. Each record contains a factor and a target. For the whole dataset we can calculate the result of our inference - mean value of target and it minimize MSE and Logloss. Then we can try to split our dataset into two parts, so we can infer value for the one part and for the another separately. It gives one more degree of freedom and allows to make our loss lower. Gain is the difference between out loss on initial dataset and the sum of losses over two splitted datasets.</p>
<p>In order to solve this problem for a generic loss function, one can use Tailor approximation and the fact that the inference value is one for the whole dataset</p>
<figure class="eq-illustration">
  <img src="https://tarstars.github.io/telepub/good_looking/assets/parabolic-minimum.webp" alt="Quadratic approximation of the loss surface" />
</figure>
<p>The key expression x0 = (l&#x27;) / (l&#x27;&#x27;) is surprisingly simple. It means that we can make Newton&#x27;s step toward the minimum, and really it&#x27;s a second-order method.</p>
<p>Now for each split we can calculate loss defect and find the split with the best defect value. There is one more interesting trick here: it&#x27;s convenient to sort each factor by its value and move through these values incrementally updating loss defect. And, basically, that is it. Vanilla GBDT in a nutshell.</p>
<p>Of course, there are some interesting things to discuss later: multithreading, contemporary standards on GBDT trees building, truncating policies, regularizations. But as idea, it&#x27;s all you need to remember:</p>
<ul>
<li>make splits by factors</li>
<li>find the optimal split using loss defect expression</li>
<li>repeat recursively to build a tree</li>
</ul>
</article>
</body>
</html>